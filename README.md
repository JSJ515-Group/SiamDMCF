SiamDMCF: A Dynamic Multi-order Context Fusion Siamese Network for Robust Visual Tracking

Siamese-based tracking methodologies have demonstrated notable efficacy in visual object tracking in recent times. Nevertheless, a persistent limitation stems from the inherent feature extraction capacity of prevalent Siamese-type networks, which hinders their ability to comprehensively differentiate targets from the surrounding background. Consequently, trackers are prone to drifting when confronted with challenges such as occlusions, variations in scale, and rapid object movement.To address this problem, We develop an original Dynamic Multi-Order Context Fusion Siamese Network for object tracking. By introducing deformable convolution and a gated aggregation mechanism, we achieve discriminative visual representation learning. We also develop an adaptive fine-grained channel cross-correlation operation, which enhances the accuracy of cross-correlation matching through dynamic weight generation and adaptive feature selection. Furthermore, we introduce a multi-semantic spatial-channel coordinated attention. This attention mechanism performs spatial-channel dual calibration on the cross-correlation generated response maps, thereby enhancing target activation. "We conducted extensive experiments on four benchmark datasets: OTB100, UAV123, GOT-10K, and LaSOT, to confirm our tracker's state-of-the-art (SOTA) performance.